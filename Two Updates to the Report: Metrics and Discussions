1. Update to the Metrics section:

What's added:

"
Detailed reasons for the specific metrics picked:

Confusion Matrix: this is the basis of the Classification Report. It gives the exact counts of each quadrant. Many times, the corporates do not directly use the indicators included in the Classification Report, instead, some other indicators derived from the Confusion Matrix are preferred. For example, the risk department of financial corporates prefer the Type 2 Error Rate as the most important KPI to measure model performance. Confusion Matrix provides the direct numbers to calculate such indicators.

Classification Report: this report provides Precision and Recall rates, which are widely used for evaluating classification models. Moreover, to better address the Precision & Recall Trade-Offs, the report also directly provides F1-Score, which is the harmonic mean of those indicators that punishes extreme values. The Classification Report is a very useful tool. Here is some more information about it: https://towardsdatascience.com/tradeoffs-how-to-aim-for-the-sweet-spot-c20b40d5e6b6

Model Loss Function: This indicator is a must have when training the Neural Network models. Because this type of models are easy to suffer from the overfitting problem, the Loss Function can put a stop sign on the road of training, and reminds "your test group has already reach the optimized status, do not go too far beyond!"
"

2. Update to the Discussions section:

What's added:

"
One thing I think definitely worth trying is to play around the number of layers and number of nodes per layer. Limited by the RAM capacity of my PC, I only specified the model to have 3 learning layers (the last output layer excluded) and each layer to have larger number of nodes. If given more computation power, I'll try to have a couple more layers with few number of nodes per layer. I'd like to see if it can indeed improve the model's predictive power, because in theory it should.
"
